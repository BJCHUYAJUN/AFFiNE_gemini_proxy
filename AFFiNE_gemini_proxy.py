from fastapi import FastAPI, Request
from starlette.responses import StreamingResponse
import aiohttp
import uvicorn
import json
import asyncio
import base64
import io
import time
import os
import re
from PIL import Image
from collections import defaultdict

app = FastAPI()

# -------------------------- æ ¸å¿ƒé…ç½® --------------------------
OLLAMA_EMBED_MODEL = "nomic-embed-text:latest"
OLLAMA_CHAT_MODEL = "deepseek-r1:14b"
OLLAMA_URL = "http://192.168.2.246:11434"
MAX_RETRIES = 3
REQUEST_TIMEOUT = 60
TEMP_IMAGE_DIR = "/tmp/affine_images"
CONVERSATION_HISTORY = defaultdict(list)
os.makedirs(TEMP_IMAGE_DIR, exist_ok=True)
os.chmod(TEMP_IMAGE_DIR, 0o777)
# --------------------------------------------------------------------------------

# -------------------------- å›¾ç‰‡å¤„ç†ï¼ˆå¼ºåˆ¶è§£æï¼‰--------------------------
def is_base64(s: str) -> bool:
    """åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºBase64æ•°æ®"""
    try:
        return len(s) % 4 == 0 and re.match('^[A-Za-z0-9+/]+[=]*$', s) is not None
    except:
        return False

def save_base64_image(base64_data: str) -> str:
    """å¼ºåˆ¶è§£æä»»ä½•å¯èƒ½çš„Base64å›¾ç‰‡æ•°æ®"""
    try:
        if "," in base64_data:
            base64_data = base64_data.split(",", 1)[1]
        padding = len(base64_data) % 4
        if padding != 0:
            base64_data += "=" * (4 - padding)
        
        if not is_base64(base64_data):
            raise ValueError("ä¸æ˜¯æœ‰æ•ˆçš„Base64æ•°æ®")
        
        image_bytes = base64.b64decode(base64_data)
        with Image.open(io.BytesIO(image_bytes)) as img:
            if img.mode in ("RGBA", "P"):
                img = img.convert("RGB")
            img.thumbnail((1024, 1024))
            image_name = f"final_img_{int(time.time() * 1000)}.jpg"
            image_path = os.path.join(TEMP_IMAGE_DIR, image_name)
            img.save(image_path, "JPEG", quality=80)
        
        print(f"âœ… å›¾ç‰‡å¤„ç†æˆåŠŸï¼š{image_path}ï¼ˆå°ºå¯¸ï¼š{img.size}ï¼Œå¤§å°ï¼š{os.path.getsize(image_path)/1024:.1f}KBï¼‰")
        return image_path
    except Exception as e:
        print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥ï¼š{str(e)}")
        raise

# -------------------------- ä¼šè¯ç®¡ç†ï¼ˆç¡®ä¿ç¨³å®šï¼‰--------------------------
async def get_session_id(request: Request) -> str:  # æ”¹ä¸ºå¼‚æ­¥å‡½æ•°
    """ä¼˜å…ˆä»è¯·æ±‚ä½“æå–ä¼šè¯IDï¼Œç¡®ä¿åŒä¸€å¯¹è¯IDä¸å˜"""
    # 1. å°è¯•ä»è¯·æ±‚ä½“æå–ï¼ˆæœ€å¯é ï¼‰
    try:
        body = json.loads(await request.body())  # ç°åœ¨å¯ä»¥åœ¨å¼‚æ­¥å‡½æ•°ä¸­ä½¿ç”¨awaitäº†
        session_id = body.get("session_id") or body.get("conversation_id")
        if session_id:
            return str(session_id)
    except:
        pass
    
    # 2. å°è¯•ä» headers/params æå–
    session_id = request.headers.get("X-Session-ID") or request.query_params.get("session_id")
    if session_id:
        return session_id
    
    # 3. æœ€åç”¨å®¢æˆ·ç«¯IP+ç”¨æˆ·ä»£ç†ç”Ÿæˆç¨³å®šID
    client_ip = request.client.host
    user_agent = request.headers.get("User-Agent", "unknown")
    return f"stable_{hash(f'{client_ip}_{user_agent}') % 100000}"

def update_conversation_history(session_id: str, role: str, content: str, image_path: str = None):
    """å¼ºåˆ¶ä¿ç•™å†å²ï¼Œå¢åŠ é•¿åº¦é™åˆ¶ä¿æŠ¤"""
    history = CONVERSATION_HISTORY[session_id]
    history.append({"role": role, "content": content, "image_path": image_path, "timestamp": time.time()})
    if len(history) > 30:
        CONVERSATION_HISTORY[session_id] = history[-30:]
    print(f"ğŸ“ ä¼šè¯{session_id}å†å²é•¿åº¦ï¼š{len(CONVERSATION_HISTORY[session_id])}è½®")

# -------------------------- ä¸»è¯·æ±‚å¤„ç† --------------------------
@app.post("/v1/models/{model_path:path}")
async def proxy_request(model_path: str, request: Request):
    # æ¸…ç†è¿‡æœŸå›¾ç‰‡
    asyncio.create_task(asyncio.to_thread(lambda: [
        os.remove(os.path.join(TEMP_IMAGE_DIR, f)) 
        for f in os.listdir(TEMP_IMAGE_DIR) 
        if os.path.isfile(os.path.join(TEMP_IMAGE_DIR, f)) and 
        time.time() - os.path.getmtime(os.path.join(TEMP_IMAGE_DIR, f)) > 600
    ]))
    
    # è·å–ç¨³å®šçš„ä¼šè¯IDï¼ˆè°ƒç”¨å¼‚æ­¥å‡½æ•°éœ€è¦ç”¨awaitï¼‰
    session_id = await get_session_id(request)
    print(f"\nğŸ“Œ å¤„ç†ä¼šè¯ï¼š{session_id}ï¼Œè¯·æ±‚è·¯å¾„ï¼š{model_path}")

    # è§£æè¯·æ±‚ä½“ï¼ˆä¿ç•™åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•ï¼‰
    raw_body = await request.body()
    try:
        body = json.loads(raw_body)
    except Exception as e:
        print(f"è§£æè¯·æ±‚ä½“é”™è¯¯ï¼š{str(e)}ï¼ŒåŸå§‹æ•°æ®ï¼š{raw_body[:200]}...")
        return StreamingResponse(iter([f"æ— æ•ˆJSONï¼š{str(e)}"]), status_code=400)

    # 1. å¤„ç†åµŒå…¥è¯·æ±‚
    if "gemini-embedding-001" in model_path and "embedContent" in model_path:
        text = body.get("content", {}).get("parts", [{}])[0].get("text", "")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{OLLAMA_URL}/api/embeddings",
                    json={"model": OLLAMA_EMBED_MODEL, "prompt": text},
                    timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)
                ) as resp:
                    if resp.status == 200:
                        return {"embedding": {"values": (await resp.json()).get("embedding", [])}}
                    error = await resp.text()
                    print(f"åµŒå…¥è¯·æ±‚å¤±è´¥ï¼š{error}")
                    return StreamingResponse(iter([f"åµŒå…¥é”™è¯¯ï¼š{error}"]), status_code=resp.status)
        except Exception as e:
            return StreamingResponse(iter([f"åµŒå…¥è¯·æ±‚å¼‚å¸¸ï¼š{str(e)}"]), status_code=500)

    # 2. å¤„ç†å¤šæ¨¡æ€èŠå¤©ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰
    elif "gemini-2.5-flash" in model_path and "streamGenerateContent" in model_path:
        try:
            current_text = ""
            current_image_path = None
            gemini_messages = body.get("contents", [])

            for msg in gemini_messages:
                for part in msg.get("parts", []):
                    if isinstance(part, dict) and "text" in part:
                        current_text += str(part["text"]) + "\n"
                    
                    if isinstance(part, dict):
                        for key, value in part.items():
                            if (any(k in key.lower() for k in ["image", "media", "img", "pic"]) and 
                                isinstance(value, str) and len(value) > 500):
                                print(f"ğŸ” å‘ç°ç–‘ä¼¼å›¾ç‰‡å­—æ®µï¼š{key}ï¼ˆé•¿åº¦ï¼š{len(value)}ï¼‰")
                                try:
                                    current_image_path = save_base64_image(value)
                                    break
                                except:
                                    print(f"âš ï¸  å­—æ®µ{key}ä¸æ˜¯æœ‰æ•ˆå›¾ç‰‡ï¼Œç»§ç»­æ£€æµ‹")
                        
                        if not current_image_path and "data" in part:
                            data = part["data"]
                            if isinstance(data, str) and len(data) > 500:
                                print(f"ğŸ” æ£€æµ‹åˆ°dataå­—æ®µï¼Œå°è¯•è§£æå›¾ç‰‡...")
                                try:
                                    current_image_path = save_base64_image(data)
                                except:
                                    pass

            current_text = current_text.strip() or "è¯·åˆ†æå›¾ç‰‡å†…å®¹"
            update_conversation_history(session_id, "user", current_text, current_image_path)

            full_history = []
            for msg in CONVERSATION_HISTORY[session_id]:
                content = msg["content"]
                if msg["image_path"]:
                    content += "\nã€è¯¥æ¶ˆæ¯åŒ…å«å›¾ç‰‡ï¼Œè¯·ç»“åˆå›¾ç‰‡å†…å®¹å›ç­”ã€‘"
                full_history.append({"role": msg["role"], "content": content})

            ollama_body = {
                "model": OLLAMA_CHAT_MODEL,
                "messages": full_history,
                "stream": True,
                "timeout": REQUEST_TIMEOUT,
                "options": {"num_ctx": 4096}
            }
            if current_image_path:
                ollama_body["image"] = current_image_path
                print(f"ğŸš€ å‘é€å¸¦å›¾ç‰‡è¯·æ±‚ï¼šæ¨¡å‹={OLLAMA_CHAT_MODEL}ï¼Œå†å²={len(full_history)}è½®")
            else:
                print(f"ğŸš€ å‘é€æ–‡æœ¬è¯·æ±‚ï¼šå†å²={len(full_history)}è½®ï¼ˆè‹¥ä¸Šä¼ äº†å›¾ç‰‡åˆ™æœªæ£€æµ‹åˆ°ï¼‰")

            async def sse_generator():
                model_reply = ""
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(f"{OLLAMA_URL}/api/chat", json=ollama_body) as resp:
                            print(f"LLaVAå“åº”çŠ¶æ€ï¼š{resp.status}")
                            if resp.status != 200:
                                error = await resp.text()
                                yield f"data: {json.dumps({'error': f'æ¨¡å‹é”™è¯¯ï¼š{error}'})}\n\n"
                                return

                            async for line in resp.content:
                                if line:
                                    line_str = line.decode().strip()
                                    if not line_str:
                                        continue
                                    try:
                                        ollama_line = json.loads(line_str)
                                    except:
                                        yield f"data: {json.dumps({'error': 'å“åº”æ ¼å¼é”™è¯¯'})}\n\n"
                                        continue

                                    content = ollama_line.get("message", {}).get("content", "")
                                    done = ollama_line.get("done", False)
                                    if content:
                                        model_reply += content
                                        sse_data = json.dumps({
                                            "streamType": "text",
                                            "candidates": [{
                                                "content": {"parts": [{"text": content}], "role": "model"},
                                                "finishReason": None
                                            }]
                                        })
                                        yield f"data: {sse_data}\n\n"
                                    
                                    if done:
                                        update_conversation_history(session_id, "model", model_reply.strip())
                                        yield "data: [DONE]\n\n"
                                        break
                except Exception as e:
                    yield f"data: {json.dumps({'error': f'å¤„ç†é”™è¯¯ï¼š{str(e)}'})}\n\n"
                finally:
                    if current_image_path and os.path.exists(current_image_path):
                        os.remove(current_image_path)
                        print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶å›¾ç‰‡ï¼š{current_image_path}")

            return StreamingResponse(
                content=sse_generator(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )
        except Exception as e:
            error = f"èŠå¤©å¤„ç†é”™è¯¯ï¼š{str(e)}"
            print(error)
            return StreamingResponse(iter([error]), status_code=500)

    return StreamingResponse(iter([f"è·¯å¾„æœªæ‰¾åˆ°ï¼š{model_path}"]), status_code=404)

if __name__ == "__main__":
    print(f"ğŸš€ å¯åŠ¨æœ€ç»ˆç‰ˆå¤šæ¨¡æ€ä»£ç†ï¼ˆè§£å†³å›¾ç‰‡å’Œä¸Šä¸‹æ–‡é—®é¢˜ï¼‰ï¼Œæ¨¡å‹ï¼š{OLLAMA_CHAT_MODEL}")
    uvicorn.run(app, host="0.0.0.0", port=4000, log_level="info")
